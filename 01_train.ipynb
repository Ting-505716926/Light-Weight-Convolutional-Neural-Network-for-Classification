{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b646aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Layer.CSP_MB_Layers import *\n",
    "from utils.DataGenerator import load_data\n",
    "from model import *\n",
    "from models.config import CustomizeSmall, CustomizeLarge, MobileNetLarge, MobileNetSamll, EfficientNetB0\n",
    "from utils.Flops import get_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb2f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型參數設定\n",
    "# MobileNetV3Large、 MobileNetV3Small、 EfficientNetB0、 CustomizeLarge 、CustomizeSmall\n",
    "backbone = 'CustomizeSmall'\n",
    "# SE CBAM CA\n",
    "SE_CBAM_CA = 'SE'\n",
    "# Adam RMSprop CLR\n",
    "LR_mode = 'Adam'\n",
    "unfrozen = []\n",
    "\n",
    "# 圖片大小\n",
    "input_shape = None\n",
    "# 類別數\n",
    "num_classes = 0\n",
    "\n",
    "# 訓練參數\n",
    "BATCH_SIZE = 128\n",
    "epoch_1 = 150\n",
    "epoch_2 = 500\n",
    "Dropout_rate = 0.5\n",
    "\n",
    "# 100 Bird Species  or  325 Bird Species  or  cifar100 or cifar10\n",
    "Dataset =[\"100 Bird Species\", \"325 Bird Species\", \"cifar100\", \"cifar10\"]\n",
    "Dataset = Dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340271e",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a178491",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Dataset == \"325 Bird Species\":\n",
    "    dir = \"./Dataset/325 Bird Species/\"\n",
    "    input_shape = (224,224,3)\n",
    "    num_classes = 325\n",
    "    trainset, valset, testset = load_data(Dataset, dir, input_shape, BATCH_SIZE)\n",
    "    \n",
    "elif Dataset == \"100 Bird Species\":\n",
    "    dir = \"./Dataset/100 Bird Species/\"\n",
    "    input_shape = (224,224,3)\n",
    "    num_classes = 100\n",
    "    trainset, valset, testset = load_data(Dataset, dir, input_shape, BATCH_SIZE)\n",
    "    \n",
    "elif Dataset == \"cifar100\":\n",
    "    input_shape = (32,32,3)\n",
    "    num_classes = 100\n",
    "    datagen_train, datagen_val, x_train, y_train, x_val, y_val, x_test, y_test = load_data(Dataset, '', input_shape, num_classes)\n",
    "    \n",
    "elif Dataset == \"cifar10\":\n",
    "    input_shape = (32,32,3)\n",
    "    num_classes = 10\n",
    "    datagen_train, datagen_val, x_train, y_train, x_val, y_val, x_test, y_test = load_data(Dataset, '', input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7febb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "if  backbone == \"CustomizeLarge\":\n",
    "    Specification, unfrozen = CustomizeLarge(Dataset)\n",
    "elif backbone == \"CustomizeSmall\":\n",
    "    Specification, unfrozen = CustomizeSmall(Dataset)\n",
    "elif backbone == 'MobileNetV3Large':\n",
    "    Specification, unfrozen = MobileNetLarge(Dataset)\n",
    "elif backbone == 'MobileNetV3Small':\n",
    "    Specification, unfrozen = MobileNetSamll(Dataset)\n",
    "\n",
    "\n",
    "base_model = build_base_model(input_shape, SE_CBAM_CA, Specification)\n",
    "# base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5127c17",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predtions = predictions_head(base_model, num_classes, Dropout_rate)\n",
    "model = build_model(base_model, predtions)\n",
    "model.summary()\n",
    "print(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca2195",
   "metadata": {},
   "source": [
    "# Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11910179",
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 0.00025\n",
    "MAX_LR = 0.001\n",
    "if Dataset == \"325 Bird Species\" or Dataset == \"100 Bird Species\":\n",
    "    steps_per_epoch = trainset.samples // BATCH_SIZE\n",
    "elif Dataset == \"cifar10\" or Dataset == \"cifar100\":\n",
    "    steps_per_epoch = len(x_train) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "optimizer = optimizer_set(LR_mode=LR_mode,\n",
    "                          INIT_LR=INIT_LR,\n",
    "                          MAX_LR=MAX_LR,\n",
    "                          steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "loss = CategoricalCrossentropy(label_smoothing=0.1)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4e40a",
   "metadata": {},
   "source": [
    "# Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51135f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from utils.gradcam import GradCAM, overlay_gradCAM\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'grad-cam']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    gradCAM = GradCAM(model=model, layerName=\"Conv3hardswish\")\n",
    "    \n",
    "    if Dataset == \"325 Bird Species\" or Dataset == \"100 Bird Species\":\n",
    "        img = cv2.imread('Dataset/325 Bird Species/train\\YELLOW CACIQUE/010.jpg')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        upsample_size = (img.shape[0],img.shape[1])\n",
    "        \n",
    "        pred_img = tf.image.resize(img,(224,224))\n",
    "        pred_img = tf.reshape(pred_img, (1,224,224,3))\n",
    "        pred_img = tf.cast(pred_img, dtype=tf.float32)/255\n",
    "        \n",
    "    elif Dataset == \"cifar10\" or Dataset == \"cifar100\":\n",
    "        img = cv2.imread('Dataset/cifar10/train/13.png')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        upsample_size = (img.shape[0],img.shape[1])\n",
    "        \n",
    "        pred_img = tf.image.resize(img,(32,32))\n",
    "        pred_img = tf.reshape(pred_img, (1,32,32,3))\n",
    "        pred_img = tf.cast(pred_img, dtype=tf.float32)/255\n",
    "    \n",
    "    pred = model.predict(pred_img)\n",
    "    classIdx = pred.argmax()\n",
    "    cam3 = gradCAM.compute_heatmap(image=pred_img, classIdx=classIdx, upsample_size=upsample_size)\n",
    "    gradcam = overlay_gradCAM(img, cam3)\n",
    "    gradcam = cv2.cvtColor(gradcam, cv2.COLOR_BGR2RGB)\n",
    "    display([img, gradcam])\n",
    "    \n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    clear_output(wait=True)\n",
    "    show_predictions()\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard,EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "checkpoint_path = 'weights/{0}/{1}_{2}_{3}/'.format(backbone, SE_CBAM_CA, LR_mode, Dataset) + \"/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1, \n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True, \n",
    "                             mode='max',\n",
    "                             save_freq='epoch')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{0}/{1}_{2}_{3}/\".format(backbone, SE_CBAM_CA, LR_mode, Dataset),\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=False,\n",
    "                          update_freq=\"epoch\",\n",
    "                          profile_batch=2,\n",
    "                          embeddings_freq=0,\n",
    "                          embeddings_metadata=None,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a7ecd2",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e127d23",
   "metadata": {},
   "source": [
    "- 如果選擇的是自定義模型，則需要從頭開始訓練整個模型\n",
    "- 若選擇其他模型，則直接轉至Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c2950",
   "metadata": {},
   "source": [
    "## Training-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af4114",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Dataset == \"325 Bird Species\" or Dataset == \"100 Bird Species\":\n",
    "    history_ft = model.fit(trainset,\n",
    "                        epochs=epoch_1,\n",
    "                        validation_data=valset,\n",
    "                        callbacks = [tensorboard, checkpoint, DisplayCallback()],\n",
    "                        # verbose=0\n",
    "                        )\n",
    "elif Dataset == \"cifar10\" or Dataset == \"cifar100\":\n",
    "    history_ft = model.fit(datagen_train.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "                            epochs=epoch_1,\n",
    "                            validation_data=datagen_val.flow(x_val, y_val, batch_size=BATCH_SIZE),\n",
    "                            callbacks = [tensorboard, checkpoint, DisplayCallback()],\n",
    "                            # verbose=0\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c3363",
   "metadata": {},
   "source": [
    "# Fine tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94587e1f",
   "metadata": {},
   "source": [
    "## 設定unfrozen的Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a841d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_to_transfer_learning(base_model, unfrozen=unfrozen)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81d2f4",
   "metadata": {},
   "source": [
    "# Training-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b1ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Dataset == \"325 Bird Species\" or Dataset == \"100 Bird Species\":\n",
    "#     history_ft = model.fit(trainset,\n",
    "#                             epochs=epoch_1 + epoch_2,\n",
    "#                             initial_epoch=epoch_1,\n",
    "#                             validation_data=valset,\n",
    "#                             callbacks=callbacks)\n",
    "# elif Dataset == \"cifar10\" or Dataset == \"cifar100\":\n",
    "#     history_ft = model.fit(datagen_train.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "#                             epochs=epoch_1 + epoch_2,\n",
    "#                             initial_epoch=epoch_1,\n",
    "#                             validation_data=datagen_val.flow(x_val, y_val, batch_size=BATCH_SIZE),\n",
    "#                             callbacks=callbacks,\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e4ef2",
   "metadata": {},
   "source": [
    "# SAVE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0339a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = './weights/{0}/{1}_{2}_{3}'.format(backbone, SE_CBAM_CA, LR_mode, Dataset)\n",
    "model_num = ''\n",
    "print('Fine模型列表：')\n",
    "for a in os.listdir(path):\n",
    "    print(a)\n",
    "    model_num = a\n",
    "print('最新的model為：',model_num)\n",
    "model_num = a[:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf879d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./weights/{0}/{1}_{2}_{3}/{4}'.format(backbone,SE_CBAM_CA,LR_mode, Dataset, model_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./weights/{0}/{1}_{2}_{3}/best_model'.format(backbone, SE_CBAM_CA, LR_mode, Dataset, model_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0fe367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the SavedModel directory\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_dir = './weights/{0}/{1}_{2}_{3}/tflite/'.format(backbone, SE_CBAM_CA, LR_mode, Dataset)\n",
    "if not os.path.isdir(tflite_dir):\n",
    "    os.makedirs(tflite_dir)\n",
    "\n",
    "# Save the model.\n",
    "with open(tflite_dir + \"model.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a480b68b8c343a8999e692c3d6d2f5bf9e4da97c94672fb67f8d413f9dcb69e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tf26')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
